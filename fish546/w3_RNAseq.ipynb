{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "RNA-Seq"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Module 3- Comparing counts in libraries; Differential expression with goal of getting a nice plot. There are a few ways to go about this. Essentially you will be mapping reads to scaffold and counting (with a little statistics). I will outline a few ways to about this below."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "CLC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CLC is probably the easiest way to get a table with count data.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#video how-to run RNA-seq on CLC\n",
      "from IPython.display import YouTubeVideo\n",
      "\n",
      "YouTubeVideo('8q60pm8LrIc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "        <iframe\n",
        "            width=\"400\"\n",
        "            height=300\"\n",
        "            src=\"http://www.youtube.com/embed/8q60pm8LrIc\"\n",
        "            frameborder=\"0\"\n",
        "            allowfullscreen\n",
        "        ></iframe>\n",
        "        "
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "<IPython.lib.display.YouTubeVideo at 0x1024954d0>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once you have RNA-seq files you can take a join, but it makes more sense to \"Run an Experiment in CLC\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://eagle.fish.washington.edu/cnidarian/skitch/Set_Up_Experiment_and_CLC_Genomics_Workbench_4.0.3__Current_workspace__Default__18917CDB.png\" alt=\"Set_Up_Experiment_and_CLC_Genomics_Workbench_4.0.3__Current_workspace__Default__18917CDB.png\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://eagle.fish.washington.edu/cnidarian/skitch/CLC_Genomics_Workbench_4.0.3__Current_workspace__Default__18917D70.png\" alt=\"CLC_Genomics_Workbench_4.0.3__Current_workspace__Default__18917D70.png\"/>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#exported\n",
      "!head /Volumes/web/whale/fish546/OlyO_MalevFemale.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"Feature ID\",\"Range (original values)\",\"IQR (original values)\",\"Difference (original values)\",\"Fold Change (original values)\",\"Expression values\",\"Gene length\",\"Unique gene reads\",\"Total gene reads\",\"RPKM\",\"Means\",\"Expression values\",\"Gene length\",\"Unique gene reads\",\"Total gene reads\",\"RPKM\",\"Means\"\r\n",
        "\"Olurida_trim_nodups_v2reads_contig_1\",\"98.605\",\"98.605\",\"98.605\",\"1.854\",\"115.452\",\"1,582\",\"14,553\",\"14,553\",\"115.452\",\"115.452\",\"214.057\",\"1,582\",\"16,391\",\"16,391\",\"214.057\",\"214.057\"\r\n",
        "\"Olurida_trim_nodups_v2reads_contig_10\",\"4.683\",\"4.683\",\"4.683\",\"1.1\",\"46.926\",\"3,992\",\"14,926\",\"14,926\",\"46.926\",\"46.926\",\"51.609\",\"3,992\",\"9,972\",\"9,972\",\"51.609\",\"51.609\"\r\n",
        "\"Olurida_trim_nodups_v2reads_contig_100\",\"28.346\",\"28.346\",\"28.346\",\"1.628\",\"45.168\",\"369\",\"1,328\",\"1,328\",\"45.168\",\"45.168\",\"73.514\",\"369\",\"1,313\",\"1,313\",\"73.514\",\"73.514\"\r\n",
        "\"Olurida_trim_nodups_v2reads_contig_1000\",\"1.589\",\"1.589\",\"-1.589\",\"-1.5\",\"4.768\",\"6,017\",\"2,286\",\"2,286\",\"4.768\",\"4.768\",\"3.18\",\"6,017\",\"926\",\"926\",\"3.18\",\"3.18\"\r\n",
        "\"Olurida_trim_nodups_v2reads_contig_10000\",\"4.671\",\"4.671\",\"-4.671\",\"-1.245\",\"23.745\",\"1,472\",\"2,785\",\"2,785\",\"23.745\",\"23.745\",\"19.074\",\"1,472\",\"1,359\",\"1,359\",\"19.074\",\"19.074\"\r\n",
        "\"Olurida_trim_nodups_v2reads_contig_10001\",\"0.286\",\"0.286\",\"0.286\",\"1.178\",\"1.61\",\"904\",\"116\",\"116\",\"1.61\",\"1.61\",\"1.897\",\"904\",\"83\",\"83\",\"1.897\",\"1.897\"\r\n",
        "\"Olurida_trim_nodups_v2reads_contig_10002\",\"8.072\",\"8.072\",\"8.072\",\"1.934\",\"8.645\",\"241\",\"166\",\"166\",\"8.645\",\"8.645\",\"16.717\",\"241\",\"195\",\"195\",\"16.717\",\"16.717\"\r\n",
        "\"Olurida_trim_nodups_v2reads_contig_10003\",\"4.165\",\"4.165\",\"4.165\",\"1.303\",\"13.754\",\"490\",\"537\",\"537\",\"13.754\",\"13.754\",\"17.919\",\"490\",\"425\",\"425\",\"17.919\",\"17.919\"\r\n",
        "\"Olurida_trim_nodups_v2reads_contig_10004\",\"2.728\",\"2.728\",\"-2.728\",\"-1.257\",\"13.352\",\"2,005\",\"2,133\",\"2,133\",\"13.352\",\"13.352\",\"10.624\",\"2,005\",\"1,031\",\"1,031\",\"10.624\",\"10.624\"\r\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!q \"select c1,c8,c14 from /Volumes/web/whale/fish546/OlyO_MalevFemale.csv\" > /Volumes/web/whale/fish546/OlyO_MalevFemale_Ucount.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!q"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Usage: \r\n",
        "\tq allows performing SQL-like statements on tabular text data.\r\n",
        "\r\n",
        "\tIts purpose is to bring SQL expressive power to manipulating text data using the Linux command line.\r\n",
        "\r\n",
        "\tBasic usage is q \"<sql like query>\" where table names are just regular file names (Use - to read from standard input)\r\n",
        "        Columns are named c1..cN and delimiter can be set using the -d (or -t) option.\r\n",
        "\r\n",
        "\tAll sqlite3 SQL constructs are supported.\r\n",
        "\r\n",
        "\tExamples:\r\n",
        "\r\n",
        "          Example 1: ls -ltr * | q \"select c1,count(1) from - group by c1\"\r\n",
        "\t    This example would print a count of each unique permission string in the current folder.\r\n",
        "\r\n",
        "\t  Example 2: seq 1 1000 | q \"select avg(c1),sum(c1) from -\"\r\n",
        "\t    This example would provide the average and the sum of the numbers in the range 1 to 1000\r\n",
        "\r\n",
        "        See the help or https://github.com/harelba/q for more details.\r\n",
        "\r\n",
        "\r\n",
        "Options:\r\n",
        "  -h, --help            show this help message and exit\r\n",
        "  -b, --beautify        Beautify output according to actual values. Might be\r\n",
        "                        slow...\r\n",
        "  -z, --gzipped         Data is gzipped. Useful for reading from stdin. For\r\n",
        "                        files, .gz means automatic gunzipping\r\n",
        "  -d DELIMITER, --delimiter=DELIMITER\r\n",
        "                        Field delimiter. If none specified, then standard\r\n",
        "                        whitespace is used as a delimiter\r\n",
        "  -t, --tab-delimited-with-header\r\n",
        "                        Same as -d <tab> -H 1. Just a shorthand for handling\r\n",
        "                        standard tab delimited file with one header line at\r\n",
        "                        the beginning of the file\r\n",
        "  -H HEADER_SKIP, --header-skip=HEADER_SKIP\r\n",
        "                        Skip n lines at the beginning of the data (still takes\r\n",
        "                        those lines into account in terms of structure)\r\n",
        "  -f FORMATTING, --formatting=FORMATTING\r\n",
        "                        Output-level formatting, in the format X=fmt,Y=fmt\r\n",
        "                        etc, where X,Y are output column numbers (e.g. 1 for\r\n",
        "                        first SELECT column etc.\r\n",
        "  -e ENCODING, --encoding=ENCODING\r\n",
        "                        Input file encoding. Defaults to UTF-8. set to none\r\n",
        "                        for not setting any encoding - faster, but at your own\r\n",
        "                        risk...\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head /Volumes/web/whale/fish546/OlyO_MalevFemale_Ucount.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"Feature values)\",\"Fold gene\r\n",
        "\"Olurida_trim_nodups_v2reads_contig_1\",\"98.605\",\"98.605\",\"98.605\",\"1.854\",\"115.452\",\"1,582\",\"14,553\",\"14,553\",\"115.452\",\"115.452\",\"214.057\",\"1,582\",\"16,391\",\"16,391\",\"214.057\",\"214.057\"  \r\n",
        "\"Olurida_trim_nodups_v2reads_contig_10\",\"4.683\",\"4.683\",\"4.683\",\"1.1\",\"46.926\",\"3,992\",\"14,926\",\"14,926\",\"46.926\",\"46.926\",\"51.609\",\"3,992\",\"9,972\",\"9,972\",\"51.609\",\"51.609\"  \r\n",
        "\"Olurida_trim_nodups_v2reads_contig_100\",\"28.346\",\"28.346\",\"28.346\",\"1.628\",\"45.168\",\"369\",\"1,328\",\"1,328\",\"45.168\",\"45.168\",\"73.514\",\"369\",\"1,313\",\"1,313\",\"73.514\",\"73.514\"  \r\n",
        "\"Olurida_trim_nodups_v2reads_contig_1000\",\"1.589\",\"1.589\",\"-1.589\",\"-1.5\",\"4.768\",\"6,017\",\"2,286\",\"2,286\",\"4.768\",\"4.768\",\"3.18\",\"6,017\",\"926\",\"926\",\"3.18\",\"3.18\"  \r\n",
        "\"Olurida_trim_nodups_v2reads_contig_10000\",\"4.671\",\"4.671\",\"-4.671\",\"-1.245\",\"23.745\",\"1,472\",\"2,785\",\"2,785\",\"23.745\",\"23.745\",\"19.074\",\"1,472\",\"1,359\",\"1,359\",\"19.074\",\"19.074\"  \r\n",
        "\"Olurida_trim_nodups_v2reads_contig_10001\",\"0.286\",\"0.286\",\"0.286\",\"1.178\",\"1.61\",\"904\",\"116\",\"116\",\"1.61\",\"1.61\",\"1.897\",\"904\",\"83\",\"83\",\"1.897\",\"1.897\"  \r\n",
        "\"Olurida_trim_nodups_v2reads_contig_10002\",\"8.072\",\"8.072\",\"8.072\",\"1.934\",\"8.645\",\"241\",\"166\",\"166\",\"8.645\",\"8.645\",\"16.717\",\"241\",\"195\",\"195\",\"16.717\",\"16.717\"  \r\n",
        "\"Olurida_trim_nodups_v2reads_contig_10003\",\"4.165\",\"4.165\",\"4.165\",\"1.303\",\"13.754\",\"490\",\"537\",\"537\",\"13.754\",\"13.754\",\"17.919\",\"490\",\"425\",\"425\",\"17.919\",\"17.919\"  \r\n",
        "\"Olurida_trim_nodups_v2reads_contig_10004\",\"2.728\",\"2.728\",\"-2.728\",\"-1.257\",\"13.352\",\"2,005\",\"2,133\",\"2,133\",\"13.352\",\"13.352\",\"10.624\",\"2,005\",\"1,031\",\"1,031\",\"10.624\",\"10.624\"  \r\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "IPlant"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://eagle.fish.washington.edu/cnidarian/skitch/_1__Discovery_Environment_188B2E8C.png\" alt=\"_1__Discovery_Environment_188B2E8C_png\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After uploading Data I used Tophat to align the PE reads to the transcriptome. The primary product is a set of BAM files. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://eagle.fish.washington.edu/cnidarian/skitch/_14__Discovery_Environment_188DB212.png\" alt=\"_14__Discovery_Environment_188DB212_png\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In theory Cuffdiff is the next step in the pipeline but I have yet to get to run without an error"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Galaxy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}